---
title: æ¨¡å‹é‡åŒ–
icon: compress
---

# æ¨¡å‹é‡åŒ–

é‡åŒ–æ˜¯å°†æ¨¡å‹å‚æ•°ä»é«˜ç²¾åº¦è½¬æ¢ä¸ºä½ç²¾åº¦çš„æŠ€æœ¯ï¼Œå¯ä»¥å¤§å¹…å‡å°‘æ¨¡å‹ä½“ç§¯å’Œæ¨ç†æˆæœ¬ã€‚

## ğŸ“Œ ä»€ä¹ˆæ˜¯é‡åŒ–

é‡åŒ–ï¼ˆQuantizationï¼‰æ˜¯æŒ‡å°†æ¨¡å‹çš„æƒé‡å’Œ/æˆ–æ¿€æ´»å€¼ä»é«˜ç²¾åº¦ï¼ˆå¦‚ FP32ã€FP16ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦ï¼ˆå¦‚ INT8ã€INT4ï¼‰çš„è¿‡ç¨‹ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦é‡åŒ–ï¼Ÿ

| é—®é¢˜ | é‡åŒ–å¦‚ä½•è§£å†³ |
|------|-------------|
| å†…å­˜å ç”¨å¤§ | é™ä½æ•°å€¼ç²¾åº¦ï¼Œå‡å°‘å†…å­˜éœ€æ±‚ |
| æ¨ç†é€Ÿåº¦æ…¢ | ä½ç²¾åº¦è®¡ç®—æ›´å¿« |
| éƒ¨ç½²æˆæœ¬é«˜ | å¯ä»¥åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œ |

### ç²¾åº¦å¯¹æ¯”

| ç²¾åº¦ | ä½æ•° | æ¯å‚æ•°å†…å­˜ | å…¸å‹ç”¨é€” |
|------|------|-----------|----------|
| FP32 | 32 bit | 4 bytes | è®­ç»ƒ |
| FP16/BF16 | 16 bit | 2 bytes | æ··åˆç²¾åº¦è®­ç»ƒ |
| INT8 | 8 bit | 1 byte | æ¨ç† |
| INT4 | 4 bit | 0.5 byte | è¾¹ç¼˜éƒ¨ç½² |

## ğŸ”§ é‡åŒ–æ–¹æ³•

### 1. è®­ç»ƒåé‡åŒ– (PTQ)

Post-Training Quantizationï¼Œåœ¨è®­ç»ƒå®Œæˆåç›´æ¥é‡åŒ–ã€‚

**ä¼˜ç‚¹**ï¼šç®€å•å¿«é€Ÿï¼Œæ— éœ€é‡æ–°è®­ç»ƒ
**ç¼ºç‚¹**ï¼šå¯èƒ½æœ‰ç²¾åº¦æŸå¤±

```python
# ä½¿ç”¨ bitsandbytes è¿›è¡Œ INT8 é‡åŒ–
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_8bit=True,
    device_map="auto"
)
```

### 2. é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)

Quantization-Aware Trainingï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿé‡åŒ–ã€‚

**ä¼˜ç‚¹**ï¼šç²¾åº¦æŸå¤±æ›´å°
**ç¼ºç‚¹**ï¼šéœ€è¦é‡æ–°è®­ç»ƒ

### 3. å¸¸è§é‡åŒ–æ ¼å¼

#### GPTQ
- GPU å‹å¥½çš„é‡åŒ–æ–¹æ³•
- æ”¯æŒ 2/3/4/8 bit
- éœ€è¦æ ¡å‡†æ•°æ®é›†

```python
from transformers import AutoModelForCausalLM, GPTQConfig

quantization_config = GPTQConfig(
    bits=4,
    dataset="c4",
    tokenizer=tokenizer
)

model = AutoModelForCausalLM.from_pretrained(
    "model_path",
    quantization_config=quantization_config,
    device_map="auto"
)
```

#### AWQ (Activation-aware Weight Quantization)
- ä¿æŠ¤é‡è¦æƒé‡çš„é‡åŒ–æ–¹æ³•
- ç²¾åº¦æŸå¤±æ›´å°
- æ¨ç†é€Ÿåº¦å¿«

#### GGUF/GGML
- llama.cpp ä½¿ç”¨çš„æ ¼å¼
- CPU å‹å¥½
- å¹¿æ³›ç”¨äºæœ¬åœ°éƒ¨ç½²

```bash
# ä½¿ç”¨ llama.cpp è¿è¡Œ GGUF æ¨¡å‹
./main -m model.Q4_K_M.gguf -p "Hello"
```

## ğŸ“Š é‡åŒ–çº§åˆ«å¯¹æ¯”

ä»¥ Llama 2 7B ä¸ºä¾‹ï¼š

| é‡åŒ– | æ¨¡å‹å¤§å° | å†…å­˜éœ€æ±‚ | ç›¸å¯¹è´¨é‡ |
|------|----------|----------|----------|
| FP16 | 14 GB | ~16 GB | 100% |
| INT8 | 7 GB | ~8 GB | ~99% |
| INT4 | 3.5 GB | ~4 GB | ~95% |
| 2-bit | 1.75 GB | ~2 GB | ~85% |

## ğŸ› ï¸ å®è·µæŒ‡å—

### ä½¿ç”¨ Ollama

```bash
# è‡ªåŠ¨ä¸‹è½½é‡åŒ–ç‰ˆæœ¬
ollama pull llama2:7b-q4_0  # 4-bit é‡åŒ–

# æŸ¥çœ‹å¯ç”¨é‡åŒ–ç‰ˆæœ¬
ollama show llama2
```

### ä½¿ç”¨ HuggingFace Transformers

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit é‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=quantization_config,
    device_map="auto"
)
```

### ä½¿ç”¨ llama.cpp

```bash
# è½¬æ¢æ¨¡å‹ä¸º GGUF æ ¼å¼
python convert.py model_path --outtype f16 --outfile model.gguf

# é‡åŒ–
./quantize model.gguf model-q4_k_m.gguf Q4_K_M
```

## ğŸ’¡ é€‰æ‹©å»ºè®®

| åœºæ™¯ | æ¨èé‡åŒ– | ç†ç”± |
|------|----------|------|
| é«˜è´¨é‡æ¨ç† | INT8 | ç²¾åº¦æŸå¤±å° |
| æ¶ˆè´¹çº§ GPU | INT4 (NF4) | å¹³è¡¡è´¨é‡å’Œæ•ˆç‡ |
| CPU æ¨ç† | GGUF Q4_K_M | llama.cpp ä¼˜åŒ– |
| ç§»åŠ¨ç«¯éƒ¨ç½² | INT4/2-bit | æè‡´å‹ç¼© |
| å¾®è°ƒ | QLoRA (NF4) | ä½æ˜¾å­˜å¾®è°ƒ |

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ç²¾åº¦æŸå¤±**ï¼šé‡åŒ–è¶Šæ¿€è¿›ï¼Œç²¾åº¦æŸå¤±è¶Šå¤§
2. **ä»»åŠ¡æ•æ„Ÿæ€§**ï¼šæ•°å­¦æ¨ç†ç­‰ä»»åŠ¡å¯¹é‡åŒ–æ›´æ•æ„Ÿ
3. **æ ¡å‡†æ•°æ®**ï¼šGPTQ ç­‰æ–¹æ³•éœ€è¦ä»£è¡¨æ€§æ•°æ®
4. **ç¡¬ä»¶æ”¯æŒ**ï¼šç¡®ä¿ç¡¬ä»¶æ”¯æŒç›®æ ‡ç²¾åº¦

## ğŸ“ˆ é‡åŒ–æ•ˆæœè¯„ä¼°

```python
# è¯„ä¼°é‡åŒ–æ¨¡å‹
from lm_eval import evaluator

results = evaluator.simple_evaluate(
    model="hf",
    model_args="pretrained=path/to/quantized/model",
    tasks=["hellaswag", "mmlu"],
)
```

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [ä»€ä¹ˆæ˜¯ LLM](/llm/what-is-llm)
- [æ¨¡å‹å¾®è°ƒ](/llm/fine-tuning)
- [æ¨¡å‹éƒ¨ç½²](/llm/deployment)
