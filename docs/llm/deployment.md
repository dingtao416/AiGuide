---
title: æ¨¡å‹éƒ¨ç½²
icon: server
---

# æ¨¡å‹éƒ¨ç½²

å°†è®­ç»ƒå¥½çš„å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæä¾›ç¨³å®šé«˜æ•ˆçš„æ¨ç†æœåŠ¡ã€‚

## ğŸ“Œ éƒ¨ç½²æ¦‚è¿°

æ¨¡å‹éƒ¨ç½²æ˜¯å°† LLM ä»å¼€å‘ç¯å¢ƒè½¬ç§»åˆ°ç”Ÿäº§ç¯å¢ƒçš„è¿‡ç¨‹ï¼Œéœ€è¦è€ƒè™‘æ€§èƒ½ã€æˆæœ¬ã€å¯ç”¨æ€§ç­‰å› ç´ ã€‚

### éƒ¨ç½²æŒ‘æˆ˜

| æŒ‘æˆ˜ | æè¿° |
|------|------|
| å†…å­˜å ç”¨ | å¤§æ¨¡å‹éœ€è¦å¤§é‡ GPU æ˜¾å­˜ |
| æ¨ç†å»¶è¿Ÿ | è‡ªå›å½’ç”Ÿæˆè¾ƒæ…¢ |
| å¹¶å‘å¤„ç† | åŒæ—¶æœåŠ¡å¤šä¸ªè¯·æ±‚ |
| æˆæœ¬æ§åˆ¶ | GPU èµ„æºæ˜‚è´µ |

## ğŸš€ éƒ¨ç½²æ–¹æ¡ˆ

### 1. äº‘ç«¯ API æœåŠ¡

ç›´æ¥ä½¿ç”¨äº‘æœåŠ¡å•†æä¾›çš„ APIã€‚

**é€‚ç”¨åœºæ™¯**ï¼šå¿«é€Ÿå¯åŠ¨ã€æ— è¿ç»´è´Ÿæ‹…

```python
from openai import OpenAI

client = OpenAI(api_key="your-api-key")
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

**ä¸»æµæœåŠ¡å•†**ï¼š
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude)
- Google (Gemini)
- é˜¿é‡Œäº‘ (é€šä¹‰åƒé—®)
- ç™¾åº¦ (æ–‡å¿ƒä¸€è¨€)

### 2. è‡ªæ‰˜ç®¡éƒ¨ç½²

åœ¨è‡ªå·±çš„æœåŠ¡å™¨ä¸Šéƒ¨ç½²æ¨¡å‹ã€‚

#### vLLM

é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œæ”¯æŒ PagedAttentionã€‚

```python
from vllm import LLM, SamplingParams

# åŠ è½½æ¨¡å‹
llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")

# é…ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.95,
    max_tokens=256
)

# æ‰¹é‡æ¨ç†
prompts = ["Hello, how are you?", "What is AI?"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

å¯åŠ¨ API æœåŠ¡ï¼š
```bash
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000
```

#### Text Generation Inference (TGI)

HuggingFace çš„æ¨ç†æœåŠ¡å™¨ã€‚

```bash
# Docker éƒ¨ç½²
docker run --gpus all -p 8080:80 \
    -v /path/to/model:/model \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id /model
```

#### Ollama

æœ¬åœ°éƒ¨ç½²æœ€ç®€å•çš„æ–¹æ¡ˆã€‚

```bash
# å®‰è£…å¹¶è¿è¡Œ
ollama pull llama2
ollama serve

# API è°ƒç”¨
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello!"
}'
```

### 3. å®¹å™¨åŒ–éƒ¨ç½²

ä½¿ç”¨ Docker å’Œ Kubernetes è¿›è¡Œå®¹å™¨åŒ–éƒ¨ç½²ã€‚

```dockerfile
# Dockerfile
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

RUN pip install vllm

COPY . /app
WORKDIR /app

CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "/model", "--port", "8000"]
```

```yaml
# Kubernetes Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-server
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: llm
        image: llm-server:latest
        resources:
          limits:
            nvidia.com/gpu: 1
        ports:
        - containerPort: 8000
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹ä¼˜åŒ–

| æŠ€æœ¯ | æ•ˆæœ |
|------|------|
| é‡åŒ– | å‡å°‘å†…å­˜ï¼ŒåŠ é€Ÿæ¨ç† |
| æ¨¡å‹å‰ªæ | å‡å°‘å‚æ•°é‡ |
| çŸ¥è¯†è’¸é¦ | ç”¨å°æ¨¡å‹æ›¿ä»£å¤§æ¨¡å‹ |

### 2. æ¨ç†ä¼˜åŒ–

#### KV Cache
ç¼“å­˜æ³¨æ„åŠ›è®¡ç®—ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—ã€‚

#### Continuous Batching
åŠ¨æ€æ‰¹å¤„ç†ï¼Œæé«˜ååé‡ã€‚

```python
# vLLM è‡ªåŠ¨æ”¯æŒ continuous batching
llm = LLM(
    model="model_path",
    max_num_batched_tokens=8192
)
```

#### Speculative Decoding
ä½¿ç”¨å°æ¨¡å‹é¢„æµ‹ï¼Œå¤§æ¨¡å‹éªŒè¯ã€‚

### 3. ç¡¬ä»¶ä¼˜åŒ–

| ç¡¬ä»¶ | é€‚ç”¨åœºæ™¯ |
|------|----------|
| NVIDIA A100 | é«˜æ€§èƒ½ç”Ÿäº§ç¯å¢ƒ |
| NVIDIA T4 | æ€§ä»·æ¯”éƒ¨ç½² |
| AMD MI250 | æ›¿ä»£æ–¹æ¡ˆ |
| Apple M-series | æœ¬åœ°å¼€å‘ |

## ğŸ”§ éƒ¨ç½²æ¶æ„

### å•æœºéƒ¨ç½²

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è´Ÿè½½å‡è¡¡               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         æ¨ç†æœåŠ¡å™¨ (vLLM)            â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚     â”‚      LLM Model          â”‚     â”‚
â”‚     â”‚    (GPU Memory)         â”‚     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åˆ†å¸ƒå¼éƒ¨ç½²

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç½‘å…³ / è´Ÿè½½å‡è¡¡              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚ Node1 â”‚  â”‚ Node2 â”‚  â”‚ Node3 â”‚
â”‚ GPUÃ—4 â”‚  â”‚ GPUÃ—4 â”‚  â”‚ GPUÃ—4 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š ç›‘æ§ä¸è¿ç»´

### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | æè¿° | ç›®æ ‡ |
|------|------|------|
| Latency (TTFT) | é¦– Token å»¶è¿Ÿ | < 500ms |
| Latency (TPS) | Token ç”Ÿæˆé€Ÿåº¦ | > 30 tokens/s |
| Throughput | ååé‡ | æœ€å¤§åŒ– |
| GPU Utilization | GPU åˆ©ç”¨ç‡ | > 80% |

### ç›‘æ§å·¥å…·

```python
# Prometheus æŒ‡æ ‡ç¤ºä¾‹
from prometheus_client import Counter, Histogram

request_counter = Counter('llm_requests_total', 'Total requests')
latency_histogram = Histogram('llm_latency_seconds', 'Request latency')
```

## ğŸ’° æˆæœ¬ä¼°ç®—

### GPU äº‘æœåŠ¡ä»·æ ¼ï¼ˆå‚è€ƒï¼‰

| GPU | äº‘æœåŠ¡å•† | ä»·æ ¼/å°æ—¶ |
|-----|----------|-----------|
| A100 40GB | AWS | ~$3.5 |
| A10G | AWS | ~$1.2 |
| T4 | AWS | ~$0.5 |

### æˆæœ¬ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨é‡åŒ–æ¨¡å‹**ï¼šé™ä½ GPU éœ€æ±‚
2. **Spot å®ä¾‹**ï¼šéå…³é”®åœºæ™¯ä½¿ç”¨ç«ä»·å®ä¾‹
3. **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šæ ¹æ®è´Ÿè½½åŠ¨æ€è°ƒæ•´
4. **æ··åˆéƒ¨ç½²**ï¼šçƒ­é—¨è¯·æ±‚ç”¨ APIï¼Œé•¿å°¾è¯·æ±‚è‡ªå»º

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [æ¨¡å‹é‡åŒ–](/llm/quantization)
- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [Ollama å·¥å…·](/tools/ollama)
