---
title: LlamaIndex
icon: database
---

# LlamaIndex

LlamaIndex æ˜¯ä¸“æ³¨äºæ•°æ®ç´¢å¼•å’Œæ£€ç´¢çš„ LLM æ¡†æ¶ï¼Œç‰¹åˆ«é€‚åˆæ„å»º RAG åº”ç”¨ã€‚

## ğŸ“Œ æ¡†æ¶æ¦‚è¿°

LlamaIndexï¼ˆåŸ GPT Indexï¼‰ä¸“æ³¨äºè¿æ¥ LLM ä¸æ•°æ®æºã€‚

### æ ¸å¿ƒç‰¹ç‚¹
- **æ•°æ®è¿æ¥å™¨**ï¼šæ”¯æŒå¤šç§æ•°æ®æº
- **ç´¢å¼•ç»“æ„**ï¼šå¤šç§ç´¢å¼•ç±»å‹ä¼˜åŒ–æ£€ç´¢
- **æŸ¥è¯¢å¼•æ“**ï¼šçµæ´»çš„æŸ¥è¯¢æ¥å£
- **Agent æ”¯æŒ**ï¼šæ•°æ®é©±åŠ¨çš„ Agent

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install llama-index
pip install llama-index-llms-openai
pip install llama-index-embeddings-openai
```

### åŸºæœ¬ä½¿ç”¨

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine()

# æŸ¥è¯¢
response = query_engine.query("è¿™ä»½æ–‡æ¡£è®²äº†ä»€ä¹ˆï¼Ÿ")
print(response)
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. æ•°æ®åŠ è½½å™¨

æ”¯æŒå¤šç§æ•°æ®æºã€‚

```python
from llama_index.core import SimpleDirectoryReader
from llama_index.readers.web import SimpleWebPageReader
from llama_index.readers.database import DatabaseReader

# æ–‡ä»¶ç›®å½•
docs = SimpleDirectoryReader("./docs").load_data()

# ç½‘é¡µ
docs = SimpleWebPageReader(html_to_text=True).load_data([
    "https://example.com/page1"
])

# æ•°æ®åº“
from sqlalchemy import create_engine
engine = create_engine("sqlite:///database.db")
reader = DatabaseReader(engine=engine)
docs = reader.load_data(query="SELECT * FROM articles")
```

### 2. ç´¢å¼•ç±»å‹

```python
from llama_index.core import (
    VectorStoreIndex,
    SummaryIndex,
    TreeIndex,
    KeywordTableIndex
)

# å‘é‡ç´¢å¼• - è¯­ä¹‰æœç´¢
vector_index = VectorStoreIndex.from_documents(documents)

# æ‘˜è¦ç´¢å¼• - éå†æ‰€æœ‰èŠ‚ç‚¹
summary_index = SummaryIndex.from_documents(documents)

# æ ‘ç´¢å¼• - å±‚æ¬¡ç»“æ„
tree_index = TreeIndex.from_documents(documents)

# å…³é”®è¯ç´¢å¼• - å…³é”®è¯åŒ¹é…
keyword_index = KeywordTableIndex.from_documents(documents)
```

### 3. æŸ¥è¯¢å¼•æ“

```python
# åŸºæœ¬æŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine()

# é…ç½®å‚æ•°
query_engine = index.as_query_engine(
    similarity_top_k=5,
    response_mode="tree_summarize",
    streaming=True
)

# æµå¼å“åº”
streaming_response = query_engine.query("é—®é¢˜")
for text in streaming_response.response_gen:
    print(text, end="")
```

### 4. èŠå¤©å¼•æ“

æ”¯æŒå¤šè½®å¯¹è¯ã€‚

```python
chat_engine = index.as_chat_engine(
    chat_mode="context",
    verbose=True
)

response = chat_engine.chat("ä½ å¥½ï¼")
response = chat_engine.chat("åˆšæ‰æˆ‘é—®äº†ä»€ä¹ˆï¼Ÿ")

# æŸ¥çœ‹å†å²
print(chat_engine.chat_history)
```

## ğŸ“Š é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰èŠ‚ç‚¹è§£æ

```python
from llama_index.core.node_parser import SentenceSplitter

# è‡ªå®šä¹‰åˆ†å—
splitter = SentenceSplitter(
    chunk_size=1024,
    chunk_overlap=200
)

nodes = splitter.get_nodes_from_documents(documents)
index = VectorStoreIndex(nodes)
```

### 2. å¤šç´¢å¼•æŸ¥è¯¢

```python
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector

# åˆ›å»ºå¤šä¸ªç´¢å¼•
tech_index = VectorStoreIndex.from_documents(tech_docs)
business_index = VectorStoreIndex.from_documents(business_docs)

# è·¯ç”±æŸ¥è¯¢å¼•æ“
query_engine = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(),
    query_engine_tools=[
        QueryEngineTool.from_defaults(
            query_engine=tech_index.as_query_engine(),
            description="æŠ€æœ¯ç›¸å…³é—®é¢˜"
        ),
        QueryEngineTool.from_defaults(
            query_engine=business_index.as_query_engine(),
            description="ä¸šåŠ¡ç›¸å…³é—®é¢˜"
        )
    ]
)
```

### 3. å­é—®é¢˜æŸ¥è¯¢

è‡ªåŠ¨åˆ†è§£å¤æ‚é—®é¢˜ã€‚

```python
from llama_index.core.query_engine import SubQuestionQueryEngine

query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=[tool1, tool2, tool3]
)

response = query_engine.query(
    "æ¯”è¾ƒ2022å¹´å’Œ2023å¹´çš„ä¸šç»©ï¼Œå¹¶åˆ†æåŸå› "
)
```

### 4. Agent

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import QueryEngineTool, FunctionTool

# å®šä¹‰å·¥å…·
search_tool = QueryEngineTool.from_defaults(
    query_engine=index.as_query_engine(),
    name="search",
    description="æœç´¢æ–‡æ¡£å†…å®¹"
)

def calculate(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    return str(eval(expression))

calc_tool = FunctionTool.from_defaults(fn=calculate)

# åˆ›å»º Agent
agent = ReActAgent.from_tools(
    [search_tool, calc_tool],
    verbose=True
)

response = agent.chat("æ–‡æ¡£ä¸­æåˆ°çš„é”€å”®é¢æ˜¯å¤šå°‘ï¼Ÿä¹˜ä»¥2æ˜¯å¤šå°‘ï¼Ÿ")
```

## ğŸ”„ ä¸å‘é‡æ•°æ®åº“é›†æˆ

```python
import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext

# ä½¿ç”¨ Chroma
chroma_client = chromadb.Client()
collection = chroma_client.create_collection("my_collection")

vector_store = ChromaVectorStore(chroma_collection=collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context
)
```

## ğŸ“Š LlamaIndex vs LangChain

| ç‰¹æ€§ | LlamaIndex | LangChain |
|------|------------|-----------|
| æ ¸å¿ƒå®šä½ | æ•°æ®ç´¢å¼•ä¸æ£€ç´¢ | é€šç”¨ LLM åº”ç”¨ |
| RAG èƒ½åŠ› | â­â­â­ | â­â­ |
| Agent èƒ½åŠ› | â­â­ | â­â­â­ |
| å­¦ä¹ æ›²çº¿ | è¾ƒä½ | ä¸­ç­‰ |
| ç´¢å¼•ç±»å‹ | ä¸°å¯Œ | åŸºç¡€ |
| ç”Ÿæ€ç³»ç»Ÿ | ä¸­ç­‰ | ä¸°å¯Œ |

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [LlamaIndex å®˜æ–¹æ–‡æ¡£](https://docs.llamaindex.ai/)
- [ä»€ä¹ˆæ˜¯ RAG](/rag/what-is-rag)
- [å‘é‡æ•°æ®åº“](/rag/vector-database)
