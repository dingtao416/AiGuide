---
title: HuggingFace
icon: face-smile
---

# HuggingFace

HuggingFace æ˜¯ AI é¢†åŸŸæœ€é‡è¦çš„å¼€æºå¹³å°ï¼Œæä¾›æ¨¡å‹ã€æ•°æ®é›†å’Œå·¥å…·åº“ã€‚

## ğŸ“Œ å¹³å°æ¦‚è¿°

HuggingFace è¢«ç§°ä¸º"AI ç•Œçš„ GitHub"ï¼Œæ˜¯å¼€æº AI ç¤¾åŒºçš„æ ¸å¿ƒã€‚

### æ ¸å¿ƒæœåŠ¡
- **Hub**ï¼šæ¨¡å‹å’Œæ•°æ®é›†æ‰˜ç®¡å¹³å°
- **Transformers**ï¼šé¢„è®­ç»ƒæ¨¡å‹åº“
- **Datasets**ï¼šæ•°æ®é›†åŠ è½½åº“
- **Spaces**ï¼šAI åº”ç”¨æ‰˜ç®¡

## ğŸš€ Transformers åº“

### å®‰è£…

```bash
pip install transformers
```

### Pipeline - æœ€ç®€ç”¨æ³•

```python
from transformers import pipeline

# æƒ…æ„Ÿåˆ†æ
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
# [{'label': 'POSITIVE', 'score': 0.9998}]

# æ–‡æœ¬ç”Ÿæˆ
generator = pipeline("text-generation", model="gpt2")
text = generator("AI is", max_length=50)

# é—®ç­”
qa = pipeline("question-answering")
result = qa(question="What is AI?", context="AI stands for...")

# ç¿»è¯‘
translator = pipeline("translation_en_to_zh", model="Helsinki-NLP/opus-mt-en-zh")
result = translator("Hello, world!")
```

### ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# æ¨ç†
inputs = tokenizer("Hello, I'm a language model", return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
text = tokenizer.decode(outputs[0])
```

### å¸¸ç”¨ Pipeline

| ä»»åŠ¡ | Pipeline |
|------|----------|
| æ–‡æœ¬åˆ†ç±» | `text-classification` |
| å‘½åå®ä½“è¯†åˆ« | `ner` |
| é—®ç­” | `question-answering` |
| æ–‡æœ¬ç”Ÿæˆ | `text-generation` |
| ç¿»è¯‘ | `translation` |
| æ‘˜è¦ | `summarization` |
| å›¾åƒåˆ†ç±» | `image-classification` |
| ç›®æ ‡æ£€æµ‹ | `object-detection` |

## ğŸ“Š Datasets åº“

```python
from datasets import load_dataset

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("imdb")

# æŸ¥çœ‹æ•°æ®
print(dataset["train"][0])

# æ•°æ®å¤„ç†
def tokenize(examples):
    return tokenizer(examples["text"], truncation=True)

tokenized = dataset.map(tokenize, batched=True)
```

## ğŸ  Hub ä½¿ç”¨

### æµè§ˆæ¨¡å‹

è®¿é—® [huggingface.co/models](https://huggingface.co/models)

### ä¸Šä¼ æ¨¡å‹

```python
from huggingface_hub import login, HfApi

# ç™»å½•
login(token="your_token")

# ä¸Šä¼ 
model.push_to_hub("your-username/model-name")
tokenizer.push_to_hub("your-username/model-name")
```

### ä¸‹è½½æ¨¡å‹

```bash
# CLI
huggingface-cli download meta-llama/Llama-2-7b-hf

# Python
from huggingface_hub import snapshot_download
snapshot_download("meta-llama/Llama-2-7b-hf")
```

## ğŸ”§ æ¨¡å‹å¾®è°ƒ

```python
from transformers import Trainer, TrainingArguments

# é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    learning_rate=2e-5,
    evaluation_strategy="epoch"
)

# åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# è®­ç»ƒ
trainer.train()
```

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [HuggingFace å®˜ç½‘](https://huggingface.co/)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [ä»€ä¹ˆæ˜¯ LLM](/llm/what-is-llm)
