---
title: Ollama
icon: terminal
---

# Ollama

Ollama æ˜¯æœ¬åœ°è¿è¡Œå¤§è¯­è¨€æ¨¡å‹æœ€ç®€å•çš„å·¥å…·ï¼Œä¸€è¡Œå‘½ä»¤å³å¯è¿è¡Œ Llamaã€Mistral ç­‰æ¨¡å‹ã€‚

## ğŸ“Œ å·¥å…·æ¦‚è¿°

Ollama è®©åœ¨æœ¬åœ°è¿è¡Œ LLM å˜å¾—æå…¶ç®€å•ã€‚

### æ ¸å¿ƒç‰¹ç‚¹
- **ä¸€é”®å®‰è£…**ï¼šç®€å•çš„å®‰è£…æµç¨‹
- **æ¨¡å‹ä¸°å¯Œ**ï¼šæ”¯æŒä¸»æµå¼€æºæ¨¡å‹
- **èµ„æºé«˜æ•ˆ**ï¼šè‡ªåŠ¨é‡åŒ–ï¼Œä¼˜åŒ–å†…å­˜
- **API å…¼å®¹**ï¼šæä¾› OpenAI å…¼å®¹ API

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

**Windows**:
ä¸‹è½½ [ollama.com](https://ollama.com/download)

**Mac**:
```bash
brew install ollama
```

**Linux**:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### è¿è¡Œæ¨¡å‹

```bash
# è¿è¡Œ Llama 2
ollama run llama2

# è¿è¡Œå…¶ä»–æ¨¡å‹
ollama run mistral
ollama run codellama
ollama run llama3
```

### å¯¹è¯ç¤ºä¾‹

```
>>> ä½ å¥½ï¼
ä½ å¥½ï¼æˆ‘æ˜¯ Llamaï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ

>>> ä»‹ç»ä¸€ä¸‹è‡ªå·±
æˆ‘æ˜¯ä¸€ä¸ªç”± Meta è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹...
```

## ğŸ”§ å¸¸ç”¨å‘½ä»¤

```bash
# åˆ—å‡ºå·²ä¸‹è½½æ¨¡å‹
ollama list

# ä¸‹è½½æ¨¡å‹
ollama pull llama3

# åˆ é™¤æ¨¡å‹
ollama rm llama2

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
ollama show llama3

# å¯åŠ¨æœåŠ¡
ollama serve
```

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹

| æ¨¡å‹ | å¤§å° | å‘½ä»¤ |
|------|------|------|
| Llama 3 8B | ~4.7GB | `ollama run llama3` |
| Llama 3 70B | ~40GB | `ollama run llama3:70b` |
| Mistral 7B | ~4GB | `ollama run mistral` |
| CodeLlama | ~4GB | `ollama run codellama` |
| Phi-3 | ~2GB | `ollama run phi3` |
| Gemma 2 | ~5GB | `ollama run gemma2` |
| Qwen 2 | ~4GB | `ollama run qwen2` |
| DeepSeek | ~4GB | `ollama run deepseek-coder` |

## ğŸ”Œ API ä½¿ç”¨

### REST API

```bash
# ç”Ÿæˆ
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "ä½ å¥½",
  "stream": false
}'

# èŠå¤©
curl http://localhost:11434/api/chat -d '{
  "model": "llama3",
  "messages": [
    {"role": "user", "content": "ä½ å¥½"}
  ]
}'
```

### Python å®¢æˆ·ç«¯

```python
import ollama

# ç®€å•ç”Ÿæˆ
response = ollama.generate(model='llama3', prompt='ä½ å¥½')
print(response['response'])

# èŠå¤©
response = ollama.chat(model='llama3', messages=[
    {'role': 'user', 'content': 'ä½ å¥½ï¼'}
])
print(response['message']['content'])

# æµå¼è¾“å‡º
for chunk in ollama.chat(model='llama3', messages=messages, stream=True):
    print(chunk['message']['content'], end='', flush=True)
```

### OpenAI å…¼å®¹ API

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # ä»»æ„å€¼
)

response = client.chat.completions.create(
    model="llama3",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)
print(response.choices[0].message.content)
```

## âš™ï¸ è‡ªå®šä¹‰æ¨¡å‹

åˆ›å»º `Modelfile`:

```dockerfile
FROM llama3

# è®¾ç½®ç³»ç»Ÿæç¤º
SYSTEM ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯é¡¾é—®

# è®¾ç½®å‚æ•°
PARAMETER temperature 0.7
PARAMETER num_ctx 4096
```

æ„å»ºæ¨¡å‹:
```bash
ollama create my-assistant -f Modelfile
ollama run my-assistant
```

## ğŸ’¡ ä½¿ç”¨å»ºè®®

| å†…å­˜ | æ¨èæ¨¡å‹ |
|------|----------|
| 8GB | phi3, gemma:2b |
| 16GB | llama3:8b, mistral |
| 32GB | llama3:70b-q4 |
| 64GB+ | llama3:70b |

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Ollama å®˜ç½‘](https://ollama.com/)
- [æ¨¡å‹åº“](https://ollama.com/library)
- [æ¨¡å‹éƒ¨ç½²](/llm/deployment)
