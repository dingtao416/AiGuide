---
title: RAG è¯„ä¼°
icon: chart-line
---

# RAG è¯„ä¼°

ç³»ç»ŸåŒ–è¯„ä¼° RAG ç³»ç»Ÿçš„æ€§èƒ½ï¼Œç¡®ä¿æ£€ç´¢å’Œç”Ÿæˆè´¨é‡ã€‚

## ğŸ“Œ è¯„ä¼°ç»´åº¦

RAG ç³»ç»Ÿè¯„ä¼°ä¸»è¦å…³æ³¨ä¸‰ä¸ªæ–¹é¢ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG è¯„ä¼°æ¡†æ¶                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   æ£€ç´¢è´¨é‡    â”‚  â”‚   ç”Ÿæˆè´¨é‡    â”‚  â”‚   ç«¯åˆ°ç«¯     â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ - å¬å›ç‡     â”‚  â”‚ - å‡†ç¡®æ€§     â”‚  â”‚ - ç”¨æˆ·æ»¡æ„åº¦ â”‚   â”‚
â”‚  â”‚ - ç²¾ç¡®ç‡     â”‚  â”‚ - æµç•…åº¦     â”‚  â”‚ - ä»»åŠ¡å®Œæˆç‡ â”‚   â”‚
â”‚  â”‚ - ç›¸å…³æ€§     â”‚  â”‚ - ç›¸å…³æ€§     â”‚  â”‚ - å“åº”æ—¶é—´   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š æ£€ç´¢è¯„ä¼°

### 1. ä¼ ç»ŸæŒ‡æ ‡

#### å¬å›ç‡ (Recall)

æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å æ‰€æœ‰ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹ã€‚

```python
def recall_at_k(retrieved_docs, relevant_docs, k):
    retrieved_set = set(retrieved_docs[:k])
    relevant_set = set(relevant_docs)
    return len(retrieved_set & relevant_set) / len(relevant_set)
```

#### ç²¾ç¡®ç‡ (Precision)

æ£€ç´¢ç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹ã€‚

```python
def precision_at_k(retrieved_docs, relevant_docs, k):
    retrieved_set = set(retrieved_docs[:k])
    relevant_set = set(relevant_docs)
    return len(retrieved_set & relevant_set) / k
```

#### MRR (Mean Reciprocal Rank)

ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£æ’åçš„å€’æ•°ã€‚

```python
def mrr(retrieved_docs, relevant_docs):
    for i, doc in enumerate(retrieved_docs):
        if doc in relevant_docs:
            return 1 / (i + 1)
    return 0
```

### 2. è¯­ä¹‰ç›¸å…³æ€§

ä½¿ç”¨ LLM æˆ–äº¤å‰ç¼–ç å™¨è¯„ä¼°æ£€ç´¢æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚

```python
from sentence_transformers import CrossEncoder

model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def score_relevance(query, documents):
    pairs = [[query, doc] for doc in documents]
    scores = model.predict(pairs)
    return scores
```

## ğŸ“ ç”Ÿæˆè¯„ä¼°

### 1. å¿ å®åº¦ (Faithfulness)

ç”Ÿæˆå†…å®¹æ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œé¿å…å¹»è§‰ã€‚

```python
# ä½¿ç”¨ RAGAS è¯„ä¼°å¿ å®åº¦
from ragas.metrics import faithfulness
from ragas import evaluate

result = evaluate(
    dataset,
    metrics=[faithfulness]
)
```

### 2. ç­”æ¡ˆç›¸å…³æ€§ (Answer Relevancy)

ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ã€‚

```python
from ragas.metrics import answer_relevancy

result = evaluate(
    dataset,
    metrics=[answer_relevancy]
)
```

### 3. ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡

æ£€ç´¢å†…å®¹è¢«æœ‰æ•ˆåˆ©ç”¨çš„ç¨‹åº¦ã€‚

```python
from ragas.metrics import context_utilization

result = evaluate(
    dataset,
    metrics=[context_utilization]
)
```

## ğŸ”§ è¯„ä¼°å·¥å…·

### RAGAS

ä¸“é—¨ç”¨äº RAG è¯„ä¼°çš„æ¡†æ¶ã€‚

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)
from datasets import Dataset

# å‡†å¤‡è¯„ä¼°æ•°æ®
eval_data = {
    "question": ["ä»€ä¹ˆæ˜¯RAG?"],
    "answer": ["RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ..."],
    "contexts": [["RAGå…¨ç§°æ˜¯...", "RAGçš„ä¼˜åŠ¿æ˜¯..."]],
    "ground_truth": ["RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§..."]
}

dataset = Dataset.from_dict(eval_data)

# æ‰§è¡Œè¯„ä¼°
result = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ]
)

print(result)
```

### LangSmith

LangChain çš„è¿½è¸ªå’Œè¯„ä¼°å¹³å°ã€‚

```python
from langsmith import Client
from langsmith.evaluation import evaluate

client = Client()

# å®šä¹‰è¯„ä¼°å‡½æ•°
def relevance_evaluator(run, example):
    # è¯„ä¼°é€»è¾‘
    return {"score": 0.9}

# æ‰§è¡Œè¯„ä¼°
evaluate(
    lambda x: rag_chain.invoke(x),
    data="dataset_name",
    evaluators=[relevance_evaluator]
)
```

### DeepEval

å¦ä¸€ä¸ªæµè¡Œçš„è¯„ä¼°æ¡†æ¶ã€‚

```python
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(
    input="ä»€ä¹ˆæ˜¯RAG?",
    actual_output="RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ...",
    retrieval_context=["RAGå…¨ç§°æ˜¯...", "RAGçš„ä¼˜åŠ¿æ˜¯..."]
)

metric = AnswerRelevancyMetric()
metric.measure(test_case)
print(metric.score)
```

## ğŸ“ˆ è¯„ä¼°æµç¨‹

### 1. æ„å»ºè¯„ä¼°æ•°æ®é›†

```python
# è¯„ä¼°æ•°æ®é›†ç»“æ„
eval_dataset = [
    {
        "query": "ç”¨æˆ·é—®é¢˜",
        "ground_truth": "æ ‡å‡†ç­”æ¡ˆ",
        "relevant_docs": ["ç›¸å…³æ–‡æ¡£1", "ç›¸å…³æ–‡æ¡£2"]
    },
    # ...
]
```

### 2. è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹

```python
def evaluate_rag_system(rag_chain, eval_dataset):
    results = []
    
    for item in eval_dataset:
        # æ‰§è¡Œ RAG
        response = rag_chain.invoke(item["query"])
        
        # è®¡ç®—æŒ‡æ ‡
        result = {
            "query": item["query"],
            "response": response["answer"],
            "retrieved_docs": response["source_documents"],
            "retrieval_score": calculate_retrieval_metrics(
                response["source_documents"],
                item["relevant_docs"]
            ),
            "generation_score": calculate_generation_metrics(
                response["answer"],
                item["ground_truth"]
            )
        }
        results.append(result)
    
    return aggregate_results(results)
```

### 3. æŒç»­ç›‘æ§

```python
# ç”Ÿäº§ç¯å¢ƒç›‘æ§
import logging

def monitor_rag_quality(query, response, feedback=None):
    metrics = {
        "latency": response["latency"],
        "num_retrieved": len(response["contexts"]),
        "confidence": response.get("confidence"),
        "user_feedback": feedback
    }
    
    logging.info(f"RAG Metrics: {metrics}")
    
    # å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
    send_to_monitoring(metrics)
```

## ğŸ’¡ æœ€ä½³å®è·µ

### è¯„ä¼°å»ºè®®

| åœºæ™¯ | é‡ç‚¹æŒ‡æ ‡ |
|------|----------|
| é—®ç­”ç³»ç»Ÿ | å‡†ç¡®æ€§ã€å¿ å®åº¦ |
| å®¢æœæœºå™¨äºº | ç›¸å…³æ€§ã€ç”¨æˆ·æ»¡æ„åº¦ |
| çŸ¥è¯†æ£€ç´¢ | å¬å›ç‡ã€ç²¾ç¡®ç‡ |
| å†…å®¹ç”Ÿæˆ | æµç•…åº¦ã€ä¸€è‡´æ€§ |

### å¸¸è§é—®é¢˜

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|----------|----------|
| å¬å›ç‡ä½ | æ£€ç´¢ç­–ç•¥ä¸ä½³ | ä¼˜åŒ– Embeddingã€æ··åˆæ£€ç´¢ |
| å¹»è§‰ä¸¥é‡ | ä¸Šä¸‹æ–‡ä¸è¶³ | å¢åŠ æ£€ç´¢æ•°é‡ã€ä¼˜åŒ– Prompt |
| ç­”æ¡ˆä¸ç›¸å…³ | æ£€ç´¢è´¨é‡å·® | é‡æ’åºã€ä¼˜åŒ–åˆ†å— |

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [ä»€ä¹ˆæ˜¯ RAG](/rag/what-is-rag)
- [æ£€ç´¢ç­–ç•¥](/rag/retrieval-strategies)
- [RAGAS æ–‡æ¡£](https://docs.ragas.io/)
