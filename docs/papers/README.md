---
title: AI 经典论文解读
icon: file-lines
---

# AI 经典论文解读

精选人工智能领域的里程碑式论文，帮助你理解 AI 技术的核心原理。

## 🔥 Transformer 与 LLM

| 论文 | 年份 | 重要性 |
|------|------|--------|
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | 2017 | Transformer 架构开山之作 |
| [BERT](https://arxiv.org/abs/1810.04805) | 2018 | 预训练语言模型革命 |
| [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) | 2020 | 大模型涌现能力 |
| [InstructGPT](https://arxiv.org/abs/2203.02155) | 2022 | RLHF 对齐技术 |
| [LLaMA](https://arxiv.org/abs/2302.13971) | 2023 | 开源 LLM 里程碑 |
| [LLaMA 2](https://arxiv.org/abs/2307.09288) | 2023 | 开源可商用 LLM |

## 🎨 生成模型

| 论文 | 年份 | 重要性 |
|------|------|--------|
| [GAN](https://arxiv.org/abs/1406.2661) | 2014 | 生成对抗网络开创 |
| [VAE](https://arxiv.org/abs/1312.6114) | 2013 | 变分自编码器 |
| [Diffusion Models](https://arxiv.org/abs/2006.11239) | 2020 | DDPM 扩散模型 |
| [Stable Diffusion](https://arxiv.org/abs/2112.10752) | 2021 | 潜空间扩散模型 |

## 🧠 深度学习基础

| 论文 | 年份 | 重要性 |
|------|------|--------|
| [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) | 2012 | 深度学习复兴 |
| [ResNet](https://arxiv.org/abs/1512.03385) | 2015 | 残差连接 |
| [Batch Normalization](https://arxiv.org/abs/1502.03167) | 2015 | 训练稳定性 |
| [Dropout](https://jmlr.org/papers/v15/srivastava14a.html) | 2014 | 正则化技术 |
| [Adam Optimizer](https://arxiv.org/abs/1412.6980) | 2014 | 自适应优化器 |

## 🔍 RAG 与检索

| 论文 | 年份 | 重要性 |
|------|------|--------|
| [RAG](https://arxiv.org/abs/2005.11401) | 2020 | 检索增强生成 |
| [Dense Passage Retrieval](https://arxiv.org/abs/2004.04906) | 2020 | 密集检索 |
| [ColBERT](https://arxiv.org/abs/2004.12832) | 2020 | 高效检索模型 |
| [Self-RAG](https://arxiv.org/abs/2310.11511) | 2023 | 自适应检索 |

## 🤖 AI Agent

| 论文 | 年份 | 重要性 |
|------|------|--------|
| [ReAct](https://arxiv.org/abs/2210.03629) | 2022 | 推理+行动框架 |
| [Toolformer](https://arxiv.org/abs/2302.04761) | 2023 | 工具使用能力 |
| [AutoGPT](https://arxiv.org/abs/2306.02224) | 2023 | 自主 Agent |
| [Generative Agents](https://arxiv.org/abs/2304.03442) | 2023 | 生成式智能体 |

## 📚 论文阅读资源

### 论文平台
- [arXiv](https://arxiv.org/) - 预印本平台
- [Papers With Code](https://paperswithcode.com/) - 论文+代码
- [Semantic Scholar](https://www.semanticscholar.org/) - AI 论文搜索
- [Connected Papers](https://www.connectedpapers.com/) - 论文关系图

### 解读资源
- [李沐论文精读](https://github.com/mli/paper-reading) - 视频讲解
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - 图解 Transformer

---

> 💡 **建议**：从 "Attention Is All You Need" 开始，这是理解现代 AI 的基础。
